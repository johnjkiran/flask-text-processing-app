{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Processing \\n\n",
    "Creating Datasets in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(file_path):\n",
    "    \"\"\"\n",
    "    Read a text file, convert to lowercase, and remove stop words.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of processed sentences with stop words removed.\n",
    "    \"\"\"\n",
    "    # Load stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Read the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    processed_lines = []\n",
    "    for line in lines:\n",
    "        # Convert to lowercase\n",
    "        line = line.lower()\n",
    "        # Remove stop words\n",
    "        words = line.split()\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        processed_lines.append(' '.join(filtered_words))\n",
    "\n",
    "    return processed_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprecessed Line 1: be, be: question.\n",
      "Preprecessed Line 2: world's stage, men women merely players.\n",
      "Preprecessed Line 3: lady doth protest much, methinks.\n",
      "Preprecessed Line 4: rose name would smell sweet.\n",
      "Preprecessed Line 5: parting sweet sorrow.\n",
      "Preprecessed Line 6: all: thine self true.\n",
      "Preprecessed Line 7: course true love never run smooth.\n",
      "Preprecessed Line 8: cowards die many times deaths.\n",
      "Preprecessed Line 9: born great, achieve greatness, greatness thrust upon them.\n",
      "Preprecessed Line 10: better part valour, discretion.\n",
      "Augmented Line 1: be, be: question. be: question.\n",
      "Augmented Line 2: world's stage, men adult_female players. only stage, players.\n",
      "Augmented Line 3: much, lady protest doth protest much, methinks.\n",
      "Augmented Line 4: rosebush name would sweet. sweet. smell sweet.\n",
      "Augmented Line 5: parting Sweet Sweet sorrow. sorrow.\n",
      "Augmented Line 6: ego all: thine ego true. true.\n",
      "Augmented Line 7: tally course true love true never tally smooth.\n",
      "Augmented Line 8: cowards die times cowards many times deaths.\n",
      "Augmented Line 9: born great, born achieve greatness, great, greatness thrust upon them.\n",
      "Augmented Line 10: better better part better valour, discretion.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "def synonym_replacement(sentence, n):\n",
    "    \"\"\"\n",
    "    Replace n words in the sentence with their synonyms.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence to augment.\n",
    "        n (int): The number of words to replace with synonyms.\n",
    "\n",
    "    Returns:\n",
    "        str: The augmented sentence with synonyms replaced.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    num_replacements = min(n, len(words))  # Ensure we don't replace more words than available\n",
    "    replaced_indices = set()  # Track replaced indices to avoid duplicates\n",
    "\n",
    "    for _ in range(num_replacements):\n",
    "        while True:\n",
    "            word_to_replace = random.choice(words)\n",
    "            index = words.index(word_to_replace)  # Get the index of the word\n",
    "            \n",
    "            if index not in replaced_indices:  # Ensure we don't replace the same word\n",
    "                synonyms = wordnet.synsets(word_to_replace)\n",
    "                if synonyms:\n",
    "                    # Randomly select a synonym from the available options\n",
    "                    synonym = random.choice(synonyms[0].lemmas()).name()\n",
    "                    words[index] = synonym  # Replace the word\n",
    "                    replaced_indices.add(index)  # Mark this index as replaced\n",
    "                break  # Exit the while loop to select another word\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def random_word_insertion(sentence, n):\n",
    "    \"\"\"\n",
    "    Insert n random words into the sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence to augment.\n",
    "        n (int): The number of random words to insert.\n",
    "\n",
    "    Returns:\n",
    "        str: The augmented sentence with random words inserted.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    for _ in range(n):\n",
    "        # Randomly select a word to insert\n",
    "        random_word = random.choice(words)  # You can customize this to select from a broader vocabulary\n",
    "        insert_position = random.randint(0, len(words))  # Random position to insert the word\n",
    "        words.insert(insert_position, random_word)  # Insert the word\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Example usage\n",
    "file_path = 'sample.txt'  # Ensure 'sample.txt' exists in the working directory\n",
    "\n",
    "# Preprocess the text\n",
    "processed_text = preprocess_text(file_path)\n",
    "\n",
    "for idx, sentence in enumerate(processed_text):\n",
    "    print(f\"Preprecessed Line {idx + 1}: {sentence}\")\n",
    "\n",
    "# Augment the processed text\n",
    "augmented_text = []\n",
    "for sentence in processed_text:\n",
    "    # Perform synonym replacement\n",
    "    augmented_sentence = synonym_replacement(sentence, n=2)  # Replace 2 words with synonyms\n",
    "    # Perform random word insertion\n",
    "    augmented_sentence = random_word_insertion(augmented_sentence, n=2)  # Insert 2 random words\n",
    "    augmented_text.append(augmented_sentence)\n",
    "\n",
    "# Print the augmented text\n",
    "for idx, sentence in enumerate(augmented_text):\n",
    "    print(f\"Augmented Line {idx + 1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
